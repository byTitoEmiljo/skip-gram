{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59907138-303f-4d40-a9cc-b46b724248db",
   "metadata": {},
   "source": [
    "## **Librerias utilizadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8418416-4542-46b5-9185-da456e456376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e442ed14-8295-4711-84e2-752a18c3e892",
   "metadata": {},
   "source": [
    "## **Función para la normalización**\n",
    "\n",
    "* Elimina puntos, comas y caracteres tales como:<br>`! \\\" # $ % & ' ( ) * + , - . / : ; < = > ? [ \\\\ ] ^ _ { | } ~ \" “ ”`.\n",
    "* Convierte todas las palabras a minúsculas.\n",
    "* Elimina los casos nulos del corpus.\n",
    "* Convierte los números a palabras con el uso de la librería `num2words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f32b4f6-7368-4135-9c0e-f82b25c6c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizarCorpus(lista):\n",
    "    sinPuntosComasEtc = []\n",
    "    for palabra in lista:\n",
    "        palabraNueva = palabra\n",
    "        for caracter in palabra:\n",
    "            if (33 <= ord(caracter) <= 47\n",
    "                    or 58 <= ord(caracter) <= 63\n",
    "                    or 91 <= ord(caracter) <= 95\n",
    "                    or 123 <= ord(caracter) <= 126\n",
    "                    or 8220 <= ord(caracter) <= 8221):\n",
    "                palabraNueva = palabraNueva.replace(caracter, '')\n",
    "        sinPuntosComasEtc.append(palabraNueva)\n",
    "\n",
    "    enMinusculas = []\n",
    "    for palabra in sinPuntosComasEtc:\n",
    "        palabraNueva = palabra.lower()\n",
    "        enMinusculas.append(palabraNueva)\n",
    "\n",
    "    sinNulos = []\n",
    "    for palabra in enMinusculas:\n",
    "        if palabra != '':\n",
    "            sinNulos.append(palabra)\n",
    "\n",
    "    for id, num in enumerate(sinNulos):\n",
    "        try:\n",
    "            sinNulos[id] = int(num)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    numPalabra = []\n",
    "    for num in sinNulos:\n",
    "        palabraNueva = num\n",
    "        if isinstance(num, int):\n",
    "            palabraNueva = num2words(num, lang='es')\n",
    "        numPalabra.append(palabraNueva)\n",
    "\n",
    "    return numPalabra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63cc2e-71fa-49ee-82c4-0fd74c810b4c",
   "metadata": {},
   "source": [
    "## **Función stopWords**\n",
    "\n",
    "Utiliza el archivo \"stopwords-es.txt\", el cual contiene las palabras que no aportan nada al texto. Tokenizo este archivo en una lista para posteriormente eliminar dichas palabras si se encuentran en mi corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d6827f-f0fb-4bc9-820d-e9cba91266bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWords(lista):\n",
    "    with open('nlp/stopwords-es.txt', 'r', encoding='utf-8') as archivo:\n",
    "        stopwords = []\n",
    "        for palabra in archivo:\n",
    "            stopwords.append(palabra.strip())\n",
    "\n",
    "    sinStopwords = []\n",
    "    for palabra in lista:\n",
    "        if palabra not in stopwords:\n",
    "            sinStopwords.append(palabra)\n",
    "\n",
    "    return sinStopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3649f9-edbb-49b6-8bcd-2c79dbd02aaf",
   "metadata": {},
   "source": [
    "## **Función Enbeddings**\n",
    "\n",
    "Genera dos diccionarios:<br>\n",
    "* `palabraIndice`: contiene cada palabra del vocabulario (corpus) como llave (`key`) y como valor (`value`) el “ID” de cada una de las palabras.\n",
    "* `indicePalabra`: es el mimo caso que el anterior solo que, a la inversa, tiene cada “ID” como llave (`key`) y como valor (`value`) su palabra correspondiente.\n",
    "\n",
    "Los IDs son generados según su iteración como una secuencia de tiempo, si la palabra “lógica” está en la iteración 33 su ID será 31, si fuese la primera seria 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c38b072d-b76e-444d-bb3c-4cde3ee70cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabraIndice = {}\n",
    "indicePalabra = {}\n",
    "\n",
    "def generarEmbeddings(corpusRes, corpus):\n",
    "    global palabraIndice, indicePalabra\n",
    "    ##### Obtencion de los indices #####\n",
    "\n",
    "    palabraIndice = {}\n",
    "    indicePalabra = {}\n",
    "    for id, palabra in enumerate(corpusRes):\n",
    "        palabraIndice[palabra] = id\n",
    "        indicePalabra[id] = palabra\n",
    "\n",
    "    ##### Embeddings #####\n",
    "\n",
    "    data = []\n",
    "    vetana = 2\n",
    "    for centId, centPalabra in enumerate(corpus):\n",
    "        for tempRan in range(- vetana, vetana + 1):\n",
    "            contexId = centId + tempRan\n",
    "            if contexId < 0 or contexId >= len(corpus) or contexId == centId:\n",
    "                continue\n",
    "            contexPalabra = corpus[contexId]\n",
    "            data.append((palabraIndice[centPalabra], palabraIndice[contexPalabra]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc5252c-f9eb-40ef-a88c-cb82b751a788",
   "metadata": {},
   "source": [
    "## **Función main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce8d3c4b-6350-4ffd-9699-83b21618b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "def main():\n",
    "    global corpus\n",
    "\n",
    "    doc1 = 'nlp/Documento_ADEL.txt'\n",
    "    doc2 = 'nlp/sistemas formales.txt'\n",
    "\n",
    "    with open(doc2, 'r') as archivo:\n",
    "        tokenizacion = []\n",
    "        for linea in archivo:\n",
    "            palabras = linea.split()\n",
    "            for palabra in palabras:\n",
    "                tokenizacion.append(palabra)\n",
    "\n",
    "    normalizacion = normalizarCorpus(tokenizacion)\n",
    "\n",
    "    corpus = stopWords(normalizacion)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197769a-e339-4fde-ac6c-769a8b98d5c3",
   "metadata": {},
   "source": [
    "## **Preparación de los datos**\r\n",
    "\r\n",
    "Para que con la librería de “gensim” pueda recibir un corpus necesita tener la propiedad: <br>`corpus = [[‘palabras’, …]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67251228-f821-4519-8804-1611660a98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusPros = [corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617ab96-f340-4426-bf3c-4a3e3597fa85",
   "metadata": {},
   "source": [
    "## **Entrenamiento del Modelo**\r\n",
    "\r\n",
    "Para entrenar el modelo con las características de la librería, se tiene esta documentación correspondiente, las que me interesan para este entrenamiento son:<br>\r\n",
    "\r\n",
    "\r\n",
    "* **vector_size** (*int, optional*) – Dimensionality of the word vectors.\r\n",
    "\r\n",
    "* **window** (*int, optional*) – Maximum distance between the current and predicted word within a sentence.\r\n",
    "\r\n",
    "* **min_count** (*int, optional*) – Ignores all words with total frequency lower than this.\r\n",
    "\r\n",
    "* **sg** (*`{0, 1}`, optional*) – Training algorithm: 1 for skip-gram; otheise CBOW.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2089c029-749a-438e-81a3-6096e523997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Word2Vec(corpusPros, vector_size=len(corpus), window=2, min_count=1, sg=1, alpha=0.025, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf20934-42da-4eb3-85d0-9feec2ad692c",
   "metadata": {},
   "source": [
    "## **Guardar el Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cba54e6-5d5b-4565-aacd-08032f81fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.save(\"./skipgram.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb1251-ffbb-4074-aa46-a3634bb8bbca",
   "metadata": {},
   "source": [
    "## **Cargar el Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c374c3f-68b5-47f0-b0ba-4fcc6c37b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "cargarModelo = Word2Vec.load('./skipgram.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399e762c-1274-4a66-a4ef-c2d71c25130a",
   "metadata": {},
   "source": [
    "## **Palabra a evaluar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "157176da-2a31-4ded-85fb-8f005d543dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabra = \"lógica\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a743735-6b15-4245-8c45-d62d80064c5c",
   "metadata": {},
   "source": [
    "## **Encontrar palabras similares**\r\n",
    "\r\n",
    "Cuando se carga un modelo guardado de la librería `gensim` se necesita declara que es un modelo cargado con las propiedades necesarias, así que se usa `wv`.\r\n",
    "\r\n",
    "La razón por la cal escogí esta librería fue por que tiene la función de buscar las palabras más similares usando `most_similar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "837bda60-58f9-4876-9635-10ae4729af4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palabras más similares a 'lógica':\n",
      "\n",
      "  fce: 0.7575773596763611\n",
      "  paradigmático: 0.7423125505447388\n",
      "  obtención: 0.7280243635177612\n",
      "  anteriormente: 0.7266494035720825\n",
      "  describiremos: 0.7188148498535156\n",
      "  inferenciales: 0.6876607537269592\n",
      "  resumirse: 0.67838454246521\n",
      "  siglo: 0.6698052883148193\n",
      "  inferencia: 0.6555670499801636\n",
      "  básico: 0.6473241448402405\n"
     ]
    }
   ],
   "source": [
    "if palabra in cargarModelo.wv:\n",
    "    similares = cargarModelo.wv.most_similar(palabra)\n",
    "    print(f\"\\nPalabras más similares a '{palabra}':\\n\")\n",
    "    for similar_palabra, similitud in similares:\n",
    "        print(f\"  {similar_palabra}: {similitud}\")\n",
    "else:\n",
    "    print(f\"La palabra '{palabra}' no está en el vocabulario del modelo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ebe8c4-686e-4a0e-a653-c4f88beeb6d3",
   "metadata": {},
   "source": [
    "## **¿Por qué no uso la función para generar Embeddings?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4dc1c44-b835-4d3d-9706-8bfc2aa88658",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabraIndice = {}\n",
    "indicePalabra = {}\n",
    "\n",
    "def generarEmbeddings(corpusRes, corpus):\n",
    "    global palabraIndice, indicePalabra\n",
    "    ##### Obtencion de los indices #####\n",
    "\n",
    "    palabraIndice = {}\n",
    "    indicePalabra = {}\n",
    "    for id, palabra in enumerate(corpusRes):\n",
    "        palabraIndice[palabra] = id\n",
    "        indicePalabra[id] = palabra\n",
    "\n",
    "    ##### Embeddings #####\n",
    "\n",
    "    data = []\n",
    "    vetana = 2\n",
    "    for centId, centPalabra in enumerate(corpus):\n",
    "        for tempRan in range(- vetana, vetana + 1):\n",
    "            contexId = centId + tempRan\n",
    "            if contexId < 0 or contexId >= len(corpus) or contexId == centId:\n",
    "                continue\n",
    "            contexPalabra = corpus[contexId]\n",
    "            data.append((palabraIndice[centPalabra], palabraIndice[contexPalabra]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48bdf1a-fb49-4400-88b0-9b9d77f35205",
   "metadata": {},
   "source": [
    "Mi función crea pares de palabras basados en una ventana de contexto, pero no produce vectores de embeddings propiamente dichos, sino pares de índices de palabras. La biblioteca Gensim, por otro lado, espera texto crudo para generar embeddings usando modelos como Word2Vec.\n",
    "\n",
    "\n",
    "Para que Gensim acepte mis datos, necesitas proporcionar una lista de listas de palabras (corpus tokenizado). Gensim se encargará de generar los embeddings basados en su propio algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eaf9bc7-d800-4193-b6f7-eeeb00fda20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusRes = list(dict.fromkeys(corpus))\n",
    "\n",
    "embeddings = generarEmbeddings(corpusRes, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06df46d1-eaa3-419c-8c1b-20533ce1f447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (0, 2), (1, 0), (1, 2), (1, 3), (2, 0), (2, 1), (2, 3), (2, 4), (3, 1), (3, 2), (3, 4), (3, 5), (4, 2), (4, 3), (4, 5), (4, 6), (5, 3), (5, 4), (5, 6), (5, 7), (6, 4), (6, 5), (6, 7), (6, 8), (7, 5), (7, 6), (7, 8), (7, 9), (8, 6), (8, 7), (8, 9), (8, 10), (9, 7), (9, 8), (9, 10), (9, 11), (10, 8), (10, 9), (10, 11), (10, 12), (11, 9), (11, 10), (11, 12), (11, 13), (12, 10), (12, 11), (12, 13), (12, 7), (13, 11)...]\n"
     ]
    }
   ],
   "source": [
    "print(f'{embeddings[:50]}\\b...]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94bf9c5-93cd-47d2-8de8-5ea57f68ea67",
   "metadata": {},
   "source": [
    "Pero el concepto de como se generan los Embeddings me queda claro y aquí lo puedo documentar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0ff480d-f754-4ff7-bff1-38792abf9498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lógica fce\n",
      "lógica concepto\n",
      "fce lógica\n",
      "fce concepto\n",
      "fce sistema\n",
      "concepto lógica\n",
      "concepto fce\n",
      "concepto sistema\n",
      "concepto formal\n"
     ]
    }
   ],
   "source": [
    "cont = 1\n",
    "for i, j in embeddings:\n",
    "    print(indicePalabra[i], indicePalabra[j])\n",
    "    cont += 1\n",
    "    if cont == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c19c1-7003-4c21-b14a-7033ae04d4cb",
   "metadata": {},
   "source": [
    "## **Referencias**\n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
